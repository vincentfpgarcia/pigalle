<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">

  <head>
    <title>Project description</title>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <link rel="stylesheet" type="text/css" href="theme.css">
  </head>

  <body>

    <header>Pigalle</header>

    <nav>
      <a href="index.html">The project</a>
      <a href="index.html">Video intro</a>
      <a href="index.html">Video demo</a>
      <a href="index.html">Code demo</a>
    </nav>

    <main>
      <p>
We propose a collection of AI models built upon two fundamental principles: seamless integration and on-device model inference.
</p>

<p>
At our core, we are driven by a mission to revolutionize local inference, bridging the gap between AI expertise and app development.
</p>

<p>
Here's why our solution stands out:
</p>

<ul>

<li>
Empowering developers: With our platform, even developers with no prior AI knowledge can effortlessly utilize and experiment with complex AI models within seconds. This unlocks a world of untapped creative potential and enables the realization of innovative ideas.
</li>

<li>
Rapid iteration: By eliminating the need for cloud AI infrastructure, our solution enables companies to iterate faster, working with leaner teams without compromising on product quality.
</li>

<li>
The icing on the cake: Leveraging local inference not only ensures 0$ GPU cloud costs, enhanced scalability and inherent security but also promises reduced latency, improved carbon footprint, offline processing capabilities, and more.
</li>

</ul>

<p>
At Pigalle, our unwavering dedication lies in continually expanding and enhancing our collection of AI models. Through relentless iteration, we strive to make our models smaller, faster, and better, pushing the boundaries of what's possible.
</p>

<p>
But we won't stop there.
</p>

<p>
Our vision is ambitious: we aspire to become the world's leading experts in optimized AI models for local inference. We'll proudly introduce groundbreaking models that run seamlessly on devices, setting new standards in the industry.
</p>

<p>
Local inference is poised to shape the future of AI, and we are determined to be its standard bearer.
</p>
      
    </main>

  </body>

</html>